{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, image_dir, classes, transform=None, categories = []):\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.image_dir = image_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_annotations = {img['id']: [] for img in self.coco_data['images']}\n",
    "        for ann in self.coco_data['annotations']:\n",
    "            if ann['category_id'] in categories:\n",
    "                self.image_annotations[ann['image_id']].append(ann)\n",
    "        \n",
    "        #Identify the background image\n",
    "        filtered_images = [\n",
    "            img for img in self.coco_data['images']\n",
    "            if len(self.image_annotations[img['id']]) == 0\n",
    "        ]\n",
    "\n",
    "        #Select 100% of non included categories to be removed\n",
    "        num_to_remove = int(len(filtered_images) * 1)\n",
    "        images_to_remove = random.sample(filtered_images, num_to_remove)\n",
    "        images_to_remove_ids = {img['id'] for img in images_to_remove}\n",
    "\n",
    "        #Remove the selected images\n",
    "        self.coco_data['images'] = [\n",
    "            img for img in self.coco_data['images']\n",
    "            if img['id'] not in images_to_remove_ids\n",
    "        ]\n",
    "\n",
    "        #Remove the annotations as well\n",
    "        self.coco_data['annotations'] = [\n",
    "            ann for ann in self.coco_data['annotations']\n",
    "            if ann['image_id'] in {img['id'] for img in self.coco_data['images']}\n",
    "        ]\n",
    "\n",
    "        #Rebuild the annotations\n",
    "        self.image_annotations = {img['id']: [] for img in self.coco_data['images']}\n",
    "        for ann in self.coco_data['annotations']:\n",
    "            if ann['category_id'] in categories:\n",
    "                self.image_annotations[ann['image_id']].append(ann)\n",
    "\n",
    "        self.images = self.coco_data['images']\n",
    "        self.image_ids = [img['id'] for img in self.coco_data['images']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_data = self.images[idx]\n",
    "        image_id = image_data['id']\n",
    "        image_path = f\"{self.image_dir}/{image_data['file_name']}\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        annotations = self.image_annotations[image_id]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(int(ann['category_id']))\n",
    "        \n",
    "        if self.transform:\n",
    "            original_width, original_height = image.size\n",
    "            scale_x, scale_y = 256 / original_width, 256 / original_height\n",
    "            image = self.transform(image)\n",
    "\n",
    "            scaled_boxes = []\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box\n",
    "                scaled_boxes.append([x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y])\n",
    "            boxes = scaled_boxes\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.empty((0,4))\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64) if len(labels) > 0 else torch.empty((0,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels, \n",
    "            'image_id': image_id\n",
    "        }\n",
    "        return image, target\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "0.20.1+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = \"instances_Train.json\"\n",
    "frames_path = \"frames\"\n",
    "classes = {0: \"No_hit\", 1: \"Hit\"}\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = COCODataset(annotations_path, frames_path, classes, transforms, categories=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_indices = np.arange(len(dataset.image_ids))\n",
    "train_indices, test_indices = train_test_split(remaining_indices, test_size=0.2, random_state=10)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_boxes = max([len(item[1]['boxes']) for item in batch])\n",
    "\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    for img, target in batch:\n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "\n",
    "        if len(boxes) < max_boxes:\n",
    "            pad_size = max_boxes - len(boxes)\n",
    "            boxes = torch.cat([boxes, torch.empty(0, 4)], dim=0)\n",
    "            labels = torch.cat([labels, torch.zeros((pad_size, ), dtype=torch.int64)], dim=0)\n",
    "        \n",
    "        targets.append({\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': target['image_id']\n",
    "        })\n",
    "\n",
    "        images.append(img)\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 loaded successfully.\n",
      "Batch 1 loaded successfully.\n",
      "Batch 2 loaded successfully.\n",
      "Batch 3 loaded successfully.\n",
      "Batch 4 loaded successfully.\n",
      "Batch 5 loaded successfully.\n",
      "Batch 6 loaded successfully.\n",
      "Batch 7 loaded successfully.\n",
      "Batch 8 loaded successfully.\n",
      "Batch 9 loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 loaded successfully.\n",
      "Batch 11 loaded successfully.\n",
      "Batch 12 loaded successfully.\n",
      "Batch 13 loaded successfully.\n",
      "Batch 14 loaded successfully.\n",
      "Batch 15 loaded successfully.\n",
      "Batch 16 loaded successfully.\n",
      "Batch 17 loaded successfully.\n",
      "Batch 18 loaded successfully.\n",
      "Batch 19 loaded successfully.\n",
      "Batch 20 loaded successfully.\n",
      "Batch 21 loaded successfully.\n",
      "Batch 22 loaded successfully.\n",
      "Batch 23 loaded successfully.\n",
      "Batch 24 loaded successfully.\n",
      "Batch 25 loaded successfully.\n",
      "Batch 26 loaded successfully.\n",
      "Batch 27 loaded successfully.\n",
      "Batch 28 loaded successfully.\n",
      "Batch 29 loaded successfully.\n",
      "Batch 30 loaded successfully.\n",
      "Batch 31 loaded successfully.\n",
      "Batch 32 loaded successfully.\n",
      "Batch 33 loaded successfully.\n",
      "Batch 34 loaded successfully.\n",
      "Batch 35 loaded successfully.\n",
      "Batch 36 loaded successfully.\n",
      "Batch 37 loaded successfully.\n",
      "Batch 38 loaded successfully.\n",
      "Batch 39 loaded successfully.\n",
      "Batch 40 loaded successfully.\n",
      "Batch 41 loaded successfully.\n",
      "Batch 42 loaded successfully.\n",
      "Batch 43 loaded successfully.\n",
      "Batch 44 loaded successfully.\n",
      "Batch 45 loaded successfully.\n",
      "Batch 46 loaded successfully.\n",
      "Batch 47 loaded successfully.\n",
      "Batch 48 loaded successfully.\n",
      "Batch 49 loaded successfully.\n",
      "Batch 50 loaded successfully.\n",
      "Batch 51 loaded successfully.\n",
      "Batch 52 loaded successfully.\n",
      "Batch 53 loaded successfully.\n",
      "Batch 54 loaded successfully.\n",
      "Batch 55 loaded successfully.\n",
      "Batch 56 loaded successfully.\n",
      "Batch 57 loaded successfully.\n",
      "Batch 58 loaded successfully.\n",
      "Batch 59 loaded successfully.\n",
      "Batch 60 loaded successfully.\n",
      "Batch 61 loaded successfully.\n",
      "Batch 62 loaded successfully.\n",
      "Batch 63 loaded successfully.\n",
      "Batch 64 loaded successfully.\n",
      "Batch 65 loaded successfully.\n",
      "Batch 66 loaded successfully.\n",
      "Batch 67 loaded successfully.\n",
      "Batch 68 loaded successfully.\n",
      "Batch 69 loaded successfully.\n",
      "Batch 70 loaded successfully.\n",
      "Batch 71 loaded successfully.\n",
      "Batch 72 loaded successfully.\n",
      "Batch 73 loaded successfully.\n",
      "Batch 74 loaded successfully.\n",
      "Batch 75 loaded successfully.\n",
      "Batch 76 loaded successfully.\n",
      "Batch 77 loaded successfully.\n",
      "Batch 78 loaded successfully.\n",
      "Batch 79 loaded successfully.\n",
      "Batch 80 loaded successfully.\n",
      "Batch 81 loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx} loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatsu/miniconda3/envs/cvpython/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hatsu/miniconda3/envs/cvpython/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "num_classes = len(classes)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor.cls_score = torch.nn.Linear(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {'params': model.backbone.parameters(), 'lr': 0.001},\n",
    "    {'params': model.rpn.parameters(), 'lr': 0.01},\n",
    "    {'params': model.roi_heads.parameters(), 'lr': 0.01}\n",
    "]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.4006\n",
      "Epoch 2 training loss: 0.3915\n",
      "Epoch 3 training loss: 0.3921\n",
      "Epoch 4 training loss: 0.3916\n",
      "Epoch 5 training loss: 0.3919\n",
      "Epoch 6 training loss: 0.3949\n",
      "Epoch 7 training loss: 0.3914\n",
      "Epoch 8 training loss: 0.3919\n",
      "Epoch 9 training loss: 0.3934\n",
      "Epoch 10 training loss: 0.3939\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        #print(loss_dict)\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        train_loss += losses.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} training loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    del images, targets, loss_dict, losses\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating\n",
      "[[{'boxes': tensor([[ 32.1226, 170.7495,  85.0515, 220.8283]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.6779e-05], device='cuda:0')}], [{'boxes': tensor([[ 13.5332, 143.5452, 247.4714, 222.9352],\n",
      "        [ 36.6145, 108.2601,  65.8650, 161.7906],\n",
      "        [  9.2157, 112.4757, 160.1706, 230.3950]], device='cuda:0'), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([4.3443e-05, 1.8025e-05, 1.2827e-05], device='cuda:0')}], [{'boxes': tensor([[208.3282, 124.8509, 256.0000, 169.6499],\n",
      "        [ 26.3811,  67.0966, 249.9758, 235.1500],\n",
      "        [  0.0000,   0.7406, 256.0000,  42.0595]], device='cuda:0'), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([2.2247e-05, 1.3238e-05, 1.2421e-05], device='cuda:0')}], [{'boxes': tensor([[  0.4492,   1.7841, 245.0424,  35.6142]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.1409e-05], device='cuda:0')}], [{'boxes': tensor([[  0.0000,   3.5404, 252.3978,  33.6151]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([3.9350e-05], device='cuda:0')}], [{'boxes': tensor([[  5.6286, 142.0873, 252.9785, 223.5891],\n",
      "        [ 21.2002, 101.3841,  51.2338, 141.1891],\n",
      "        [100.5190, 107.1866, 256.0000, 233.9555],\n",
      "        [ 31.4226,  93.3428, 156.5344, 181.8869]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([2.5045e-05, 2.2411e-05, 2.1674e-05, 1.0038e-05], device='cuda:0')}], [{'boxes': tensor([[1.2996e+00, 3.6675e-02, 2.5480e+02, 4.2214e+01],\n",
      "        [5.0414e+01, 9.9040e+01, 2.5600e+02, 2.3597e+02]], device='cuda:0'), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([3.4608e-05, 1.3199e-05], device='cuda:0')}], [{'boxes': tensor([[  0.0000,   2.7859, 249.3779,  33.0655]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.4810e-05], device='cuda:0')}], [{'boxes': tensor([[111.5837,  77.7902, 256.0000, 246.3393]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([3.3109e-05], device='cuda:0')}], [{'boxes': tensor([[  9.4638,  63.3139, 256.0000, 239.3252],\n",
      "        [  0.9472,   2.5384, 241.1808,  31.9116]], device='cuda:0'), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([2.9853e-05, 1.0249e-05], device='cuda:0')}], [{'boxes': tensor([[  0.7800,   2.7562, 246.3136,  34.8640]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.4405e-05], device='cuda:0')}], [{'boxes': tensor([[  0.0000,   1.3836, 243.9898,  32.9657],\n",
      "        [ 95.7595, 112.0063, 256.0000, 231.6546]], device='cuda:0'), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([3.3361e-05, 1.0163e-05], device='cuda:0')}], [{'boxes': tensor([[  5.3398,  95.9990, 212.7605, 234.2741],\n",
      "        [  5.0222, 138.6561, 105.0467, 221.7010],\n",
      "        [ 18.0171,  73.0230, 185.7431, 187.2267],\n",
      "        [ 22.8011, 102.6385,  51.0284, 138.5791],\n",
      "        [  0.0000,   2.9356, 241.1887,  32.2924]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 1], device='cuda:0'), 'scores': tensor([5.6480e-05, 4.2756e-05, 1.7649e-05, 1.4376e-05, 1.0961e-05],\n",
      "       device='cuda:0')}], [{'boxes': tensor([[ 23.6053,  99.7664,  50.1100, 142.5770],\n",
      "        [  5.8498,  99.2882, 210.0299, 233.4609],\n",
      "        [  4.1391, 140.2512, 109.5340, 217.7289],\n",
      "        [ 11.1904, 134.9877,  60.6393, 177.9538]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([9.8713e-05, 6.6968e-05, 3.3315e-05, 2.1382e-05], device='cuda:0')}], [{'boxes': tensor([[  0.0000,   2.4558, 238.5955,  33.0510],\n",
      "        [ 65.1045, 114.3715, 255.3487, 235.8717],\n",
      "        [ 25.1614,  58.7417, 184.0661, 184.9729]], device='cuda:0'), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([2.5605e-05, 1.2453e-05, 1.0364e-05], device='cuda:0')}], [{'boxes': tensor([[ 70.2239, 100.0895, 105.0207, 137.0128]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.2879e-05], device='cuda:0')}], [{'boxes': tensor([[  0.0000,   3.2812, 250.3471,  34.3683]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.0880e-05], device='cuda:0')}], [{'boxes': tensor([[212.8818, 131.3808, 252.8185, 167.1505]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.0061e-05], device='cuda:0')}], [{'boxes': tensor([[  0.0000,   0.0000, 256.0000,  33.3770]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([1.3671e-05], device='cuda:0')}], [{'boxes': tensor([[  0.0000,   2.0636, 250.4657,  33.6403]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.0040e-05], device='cuda:0')}], [{'boxes': tensor([[ 84.6992, 104.3632, 256.0000, 229.6698]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([2.4968e-05], device='cuda:0')}]]\n",
      "[{'boxes': tensor([[ 99.4960, 151.3480, 129.0800, 173.1480]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([1487], device='cuda:0')}, {'boxes': tensor([[106.5040, 120.1600, 122.4400, 129.6760]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([579], device='cuda:0')}, {'boxes': tensor([[177.4720, 104.8920, 196.5320, 130.8640]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([2073], device='cuda:0')}, {'boxes': tensor([[100.4000, 108.7600, 121.6280, 125.1320]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([2421], device='cuda:0')}, {'boxes': tensor([[177.0400, 103.1520, 190.5360, 114.5720]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([1333], device='cuda:0')}, {'boxes': tensor([[ 90.7720, 110.2400, 109.1480, 121.4520]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([57], device='cuda:0')}, {'boxes': tensor([[ 51.8400, 110.7760,  66.5600, 123.1040]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([727], device='cuda:0')}, {'boxes': tensor([[168.2160, 129.8800, 189.5000, 142.3600]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([1119], device='cuda:0')}, {'boxes': tensor([[166.1400, 127.0400, 194.9480, 143.6640]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([1408], device='cuda:0')}, {'boxes': tensor([[187.4840, 110.1120, 205.6400, 121.4600],\n",
      "        [172.6760, 120.1440, 184.0240, 131.4920]], device='cuda:0'), 'labels': tensor([1, 1], device='cuda:0'), 'image_id': tensor([2190], device='cuda:0')}, {'boxes': tensor([[ 67.1040, 137.9560,  92.0200, 166.9200]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([1482], device='cuda:0')}, {'boxes': tensor([[124.3600, 134.3200, 140.8800, 148.7520]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([1965], device='cuda:0')}, {'boxes': tensor([[103.2320, 108.9920, 122.8520, 122.6960]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([44], device='cuda:0')}, {'boxes': tensor([[103.8560, 108.3720, 122.2280, 122.6960]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([43], device='cuda:0')}, {'boxes': tensor([[208.5360, 102.7280, 224.7760, 115.0880]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([2094], device='cuda:0')}, {'boxes': tensor([[ 96.1600, 104.2000, 114.8200, 119.1600]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([2301], device='cuda:0')}, {'boxes': tensor([[174.6560, 106.1720, 197.8120, 129.8400]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([2078], device='cuda:0')}, {'boxes': tensor([[178.2400, 106.8000, 197.4280, 128.8000]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([2077], device='cuda:0')}, {'boxes': tensor([[ 79.3600, 124.4000, 100.2400, 143.7600]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([2482], device='cuda:0')}, {'boxes': tensor([[ 78.3960, 104.0800,  96.4520, 123.5720]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([2655], device='cuda:0')}, {'boxes': tensor([[167.7000, 136.1200, 201.4360, 155.0840]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([1410], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "model.roi_heads.score_thresh = 1e-5\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        img = images[0].to(device)\n",
    "        target = targets[0]\n",
    "        target = {k: v.to(device) for k, v in target.items()}\n",
    "\n",
    "        pred = model([img])\n",
    "\n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(target)\n",
    "print(\"Finished evaluating\")\n",
    "print(all_preds)\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "Please run evaluate() first\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "#Prepare the ground truth annotations in COCO format\n",
    "coco_gt = {'images': [], 'annotations': [], 'categories': []}\n",
    "annotation_id = 1\n",
    "for i, target in enumerate(all_labels):\n",
    "    coco_gt['images'].append({'id': i})\n",
    "    for j, box in enumerate(target['boxes']):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        x, y, w, h = x1, y1, x2 - x1, y2 - y1\n",
    "        coco_gt['annotations'].append({\n",
    "            'image_id': i,\n",
    "            'bbox': [x, y, w, h],\n",
    "            'category_id': target['labels'][j].item(),\n",
    "            'id': annotation_id\n",
    "        })\n",
    "        annotation_id += 1\n",
    "\n",
    "# Save ground truth in COCO format\n",
    "with open('gt_annotations.json', 'w') as f:\n",
    "    json.dump(coco_gt, f)\n",
    "\n",
    "# Prepare predictions in COCO format\n",
    "coco_pred = {'images': [], 'annotations': []}\n",
    "prediction_id = 1\n",
    "for i, pred in enumerate(all_preds):\n",
    "    coco_pred['images'].append({'id': i})\n",
    "    for j, (box, score, label) in enumerate(zip(pred[0]['boxes'], pred[0]['scores'], pred[0]['labels'])):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        x, y, w, h = x1, y1, x2 - x1, y2 - y1\n",
    "        coco_pred['annotations'].append({\n",
    "            'image_id': i,\n",
    "            'bbox': [x, y, w, h],\n",
    "            'category_id': label.item(),\n",
    "            'score': score.item(),\n",
    "            'id': prediction_id\n",
    "        })\n",
    "        prediction_id += 1\n",
    "\n",
    "with open('pred_annotations.json', 'w') as f:\n",
    "    json.dump(coco_pred, f)\n",
    "\n",
    "coco_gt = COCO('gt_annotations.json')\n",
    "coco_pred = COCO('pred_annotations.json')\n",
    "\n",
    "coco_eval = COCOeval(coco_gt, coco_pred, iouType='bbox')\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"fight_2.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "output_path = \"result_2.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "    input_frame = transforms(pil_img).unsqueeze(0)\n",
    "\n",
    "    input_frame = input_frame.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_frame)\n",
    "    \n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "    filtered_boxes = boxes[scores > 1e-5]\n",
    "    filtered_labels = labels[scores > 1e-5]\n",
    "\n",
    "    scale_x, scale_y = width / 256, height / 256\n",
    "    filtered_boxes[:, [0, 2]] *= scale_x\n",
    "    filtered_boxes[:, [1, 3]] *= scale_y\n",
    "\n",
    "    for box, label, in zip(filtered_boxes, filtered_labels):\n",
    "        x1, y1, x2, y2 = x1, y1, x2, y2 = np.clip(box, 0, [width, height, width, height])\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)  # Draw bounding box\n",
    "        cv2.putText(frame, f'Label: {label}', (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "    \n",
    "    # Display the frame with predictions\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
