{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGP layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGP Layer\n",
    "class SGPLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SGPLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)  # Fine granularity\n",
    "        self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size=5, stride=1, padding=2)  # Coarser granularity\n",
    "        self.conv3 = nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=1, padding=3)  # Coarsest granularity\n",
    "        self.fusion = nn.Conv1d(out_channels * 3, out_channels, kernel_size=1)  # Fuse all scales\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        fine_features = self.conv1(x)\n",
    "        coarse_features = self.conv2(x)\n",
    "        coarsest_features = self.conv3(x)\n",
    "        combined = torch.cat([fine_features, coarse_features, coarsest_features], dim=1)\n",
    "        fused_features = self.fusion(combined)\n",
    "        return self.relu(fused_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punch Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunchDetectionModel(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(PunchDetectionModel, self).__init__()\n",
    "        self.sgp_layer = SGPLayer(in_channels, 128)\n",
    "        self.temporal_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, in_channels, sequence_length)\n",
    "        \"\"\"\n",
    "        sgp_out = self.sgp_layer(x)  # Apply SGP layer\n",
    "        pooled_features = self.temporal_pooling(sgp_out).squeeze(-1)  # Global temporal pooling\n",
    "        logits = self.fc(pooled_features)  # Final classification layer\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_video(video_path, output_path, model, sequence_length=100, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Annotate a video with hit predictions.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        output_path (str): Path to save the annotated video.\n",
    "        model (nn.Module): Trained punch detection model.\n",
    "        sequence_length (int): Number of frames in a sequence for the model.\n",
    "        threshold (float): Probability threshold for detecting a hit.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    model.eval()\n",
    "    frame_buffer = []\n",
    "    transform = ToTensor()  # Transform frames into tensors\n",
    "\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (224, 224))  # Resize to fit model input\n",
    "        frame_tensor = transform(resized_frame).permute(2, 0, 1)  # Convert to tensor\n",
    "        frame_buffer.append(frame_tensor)\n",
    "\n",
    "        if len(frame_buffer) == sequence_length:\n",
    "            input_tensor = torch.stack(frame_buffer).unsqueeze(0)  # Shape (1, C, L, H, W)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_tensor)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                prediction = probs[0, 1].item()\n",
    "\n",
    "            for frame_idx in range(sequence_length):\n",
    "                annotated_frame = frame_buffer[frame_idx].permute(1, 2, 0).numpy() * 255  # Convert back to image\n",
    "                annotated_frame = annotated_frame.astype(np.uint8)\n",
    "                label = \"Hit\" if prediction >= threshold else \"No Hit\"\n",
    "                color = (0, 255, 0) if label == \"Hit\" else (0, 0, 255)\n",
    "\n",
    "                cv2.putText(annotated_frame, f\"{label} ({prediction:.2f})\",\n",
    "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "                out.write(annotated_frame)\n",
    "\n",
    "            frame_buffer = []\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Annotated video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define input parameters\n",
    "    video_path = \"input_video.mp4\"\n",
    "    output_path = \"output_annotated.mp4\"\n",
    "\n",
    "    # Load trained model\n",
    "    in_channels = 3\n",
    "    num_classes = 2\n",
    "    model = PunchDetectionModel(in_channels, num_classes)\n",
    "    model.load_state_dict(torch.load(\"punch_detection_model.pth\"))  # Load your trained model weights\n",
    "\n",
    "    # Annotate the video\n",
    "    annotate_video(video_path, output_path, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
